{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"spark-devplugins Apache Spark is a highly extensible framework. It has given multiple plugin points to customize the behavior. Some of the plugins are well known. But many of them are developer oriented and are not documented. This work in spark-devplugins git repository is about demonstrating those extensions using small and meaningful examples. Every project in spark-devplugins repo demonstrates a plugin with an example. There is associated documentation for each of those plugins. In the git repo, tests for the plugins are also added. The tests not only tests the project but also demonstrates the behavior of the plugin in a detailed way. Currently, the repo is built with Spark 2.3.1. Documentation can be found here.","title":"Introduction"},{"location":"#spark-devplugins","text":"Apache Spark is a highly extensible framework. It has given multiple plugin points to customize the behavior. Some of the plugins are well known. But many of them are developer oriented and are not documented. This work in spark-devplugins git repository is about demonstrating those extensions using small and meaningful examples. Every project in spark-devplugins repo demonstrates a plugin with an example. There is associated documentation for each of those plugins. In the git repo, tests for the plugins are also added. The tests not only tests the project but also demonstrates the behavior of the plugin in a detailed way. Currently, the repo is built with Spark 2.3.1. Documentation can be found here.","title":"spark-devplugins"},{"location":"analyzer/","text":"Goal There are two goals: For a specific data source, allow queries that have an equality check on the \"id\" column. You may want to do this when your datasource is highly optimized for point queries. Disallow all types of joins except INNER joins. You may want to do this for your larger tables. Approach Apache Spark's parser generates an abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. analysis, optimizer, planner and execution. In this exercise, we inject a check rule in the analyzer. The check rule throws an exception if there is no where clause with an equality check for a parquet data source. It also throws an exception if there is a join of non-inner type. This check rule is applied by the Spark during the analysis phase. EqualityCheckSessionExtension injects the check rule EqualityCheckExtension has the code to analyze the plan and verify the filter clause and the join clause Try it out $ bin/spark-shell --jars PATH /analyzer-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.analyzer.EqualityCheckSessionExtension scala spark.sql( create table myidtable (value string, id integer) using parquet options (path '/tmp/p') ) scala spark.sql( Insert Into myidtable values ('1', 1) ) scala spark.sql( Insert Into myidtable values ('2', 2) ) scala spark.sql( select * from myidtable a left join myidtable b on a.id = b.id and a.id =1 ) Exception scala spark.sql( select * from myidtable ) Exception scala spark.sql( select * from myidtable a, myidtable b + where a.id = b.id and a.value = b.value and a.id = 1 ).collect.length res4: Int = 1 Github Here","title":"Analyzer Plugin"},{"location":"analyzer/#goal","text":"There are two goals: For a specific data source, allow queries that have an equality check on the \"id\" column. You may want to do this when your datasource is highly optimized for point queries. Disallow all types of joins except INNER joins. You may want to do this for your larger tables.","title":"Goal"},{"location":"analyzer/#approach","text":"Apache Spark's parser generates an abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. analysis, optimizer, planner and execution. In this exercise, we inject a check rule in the analyzer. The check rule throws an exception if there is no where clause with an equality check for a parquet data source. It also throws an exception if there is a join of non-inner type. This check rule is applied by the Spark during the analysis phase. EqualityCheckSessionExtension injects the check rule EqualityCheckExtension has the code to analyze the plan and verify the filter clause and the join clause","title":"Approach"},{"location":"analyzer/#try-it-out","text":"$ bin/spark-shell --jars PATH /analyzer-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.analyzer.EqualityCheckSessionExtension scala spark.sql( create table myidtable (value string, id integer) using parquet options (path '/tmp/p') ) scala spark.sql( Insert Into myidtable values ('1', 1) ) scala spark.sql( Insert Into myidtable values ('2', 2) ) scala spark.sql( select * from myidtable a left join myidtable b on a.id = b.id and a.id =1 ) Exception scala spark.sql( select * from myidtable ) Exception scala spark.sql( select * from myidtable a, myidtable b + where a.id = b.id and a.value = b.value and a.id = 1 ).collect.length res4: Int = 1","title":"Try it out"},{"location":"analyzer/#github","text":"Here","title":"Github"},{"location":"cmgr/","text":"Goal Launch Spark using your cluster manager. Approach Apache Spark supports cluster managers like YARN and Standalone by default. But it also allows you to plug-in your own cluster manager. For e.g. Snappydata has written their own cluster manager so that they can launch the executors in the same process that holds the data in memory. Ideally cluster manager is a separate process, which is responsible for launching executors when driver requests. In this project, for demonstration, instead of talking with an external cluster manager, driver launches the executor processes on the same machine based on configuration. Following things were done in this project: Added a file org.apache.spark.scheduler.ExternalClusterManager with the name of our cluster manager. This is how you plugin your class. Implemented ExternalClusterManager. The master URL for our project is dev-plugins. ModifiedCoarseGrainedSchedulerBackend which is the driver class launches CoarseGrainedExecutorBackend which is the executor class. Try it out SparkSubmit does not support other cluster managers and hence run the test with this project to try it out. Github Here","title":"Cluster Manager Plugin"},{"location":"cmgr/#goal","text":"Launch Spark using your cluster manager.","title":"Goal"},{"location":"cmgr/#approach","text":"Apache Spark supports cluster managers like YARN and Standalone by default. But it also allows you to plug-in your own cluster manager. For e.g. Snappydata has written their own cluster manager so that they can launch the executors in the same process that holds the data in memory. Ideally cluster manager is a separate process, which is responsible for launching executors when driver requests. In this project, for demonstration, instead of talking with an external cluster manager, driver launches the executor processes on the same machine based on configuration. Following things were done in this project: Added a file org.apache.spark.scheduler.ExternalClusterManager with the name of our cluster manager. This is how you plugin your class. Implemented ExternalClusterManager. The master URL for our project is dev-plugins. ModifiedCoarseGrainedSchedulerBackend which is the driver class launches CoarseGrainedExecutorBackend which is the executor class.","title":"Approach"},{"location":"cmgr/#try-it-out","text":"SparkSubmit does not support other cluster managers and hence run the test with this project to try it out.","title":"Try it out"},{"location":"cmgr/#github","text":"Here","title":"Github"},{"location":"optimizer/","text":"Goal Remove un-necessary sorting on an already sorted data source Approach Apache Spark's parser generates a abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. analysis, optimizer, planner and execution. In this exercise, we inject an optimizer rule. Let's say we have a datasource (created in this project) that is already sorted on a particular column. An order by query on this column of the datasource should not result in a sort operation. We can achieve this by adding an optimizer rule that does the following: Recognizes the global ascending sort operator checks if it has a child that is our sorted datasource, remove the sort operator Try it out $ bin/spark-shell --jars PATH /optimizer-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.optimizer.OptimizerExtension scala spark.read.format(classOf[dev.plugins.optimizer.SortedDataSource].getName).load().createTempView( sortedTable ) // sort query on sorted data set scala val df1 = spark.sql( select * from sortedTable where id % 2 = 0 order by id ) scala df1.explain == Physical Plan == *(1) Filter (isnotnull(id#0) ((id#0 % 2) = 0)) +- *(1) DataSourceV2Scan [id#0, negativeid#1], dev.plugins.optimizer.SortedReader@1fcc3461 scala Seq(( 1 , 1), ( 2 , 2), ( 3 , 3), ( 4 , 4), ( 5 , 5)).toDF( value , id ).createTempView( mymemorytable ) // sort query on non-sorted data set scala val df2 = spark.sql( select * from mymemorytable where id % 2 = 0 order by id ) scala df2.explain == Physical Plan == *(2) Sort [id#13 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(id#13 ASC NULLS FIRST, 200) +- *(1) Project [_1#9 AS value#12, _2#10 AS id#13] +- *(1) Filter ((_2#10 % 2) = 0) +- LocalTableScan [_1#9, _2#10] Github Here","title":"Optimizer Plugin"},{"location":"optimizer/#goal","text":"Remove un-necessary sorting on an already sorted data source","title":"Goal"},{"location":"optimizer/#approach","text":"Apache Spark's parser generates a abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. analysis, optimizer, planner and execution. In this exercise, we inject an optimizer rule. Let's say we have a datasource (created in this project) that is already sorted on a particular column. An order by query on this column of the datasource should not result in a sort operation. We can achieve this by adding an optimizer rule that does the following: Recognizes the global ascending sort operator checks if it has a child that is our sorted datasource, remove the sort operator","title":"Approach"},{"location":"optimizer/#try-it-out","text":"$ bin/spark-shell --jars PATH /optimizer-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.optimizer.OptimizerExtension scala spark.read.format(classOf[dev.plugins.optimizer.SortedDataSource].getName).load().createTempView( sortedTable ) // sort query on sorted data set scala val df1 = spark.sql( select * from sortedTable where id % 2 = 0 order by id ) scala df1.explain == Physical Plan == *(1) Filter (isnotnull(id#0) ((id#0 % 2) = 0)) +- *(1) DataSourceV2Scan [id#0, negativeid#1], dev.plugins.optimizer.SortedReader@1fcc3461 scala Seq(( 1 , 1), ( 2 , 2), ( 3 , 3), ( 4 , 4), ( 5 , 5)).toDF( value , id ).createTempView( mymemorytable ) // sort query on non-sorted data set scala val df2 = spark.sql( select * from mymemorytable where id % 2 = 0 order by id ) scala df2.explain == Physical Plan == *(2) Sort [id#13 ASC NULLS FIRST], true, 0 +- Exchange rangepartitioning(id#13 ASC NULLS FIRST, 200) +- *(1) Project [_1#9 AS value#12, _2#10 AS id#13] +- *(1) Filter ((_2#10 % 2) = 0) +- LocalTableScan [_1#9, _2#10]","title":"Try it out"},{"location":"optimizer/#github","text":"Here","title":"Github"},{"location":"parser/","text":"Goal Support \"blob\" keyword in Apache Spark SQL. If someone specifies a \"blob\" keyword, it should be converted that to Array[Byte] internally. Approach Apache Spark SQL parser is based on ANTLR. Here is a wonderful tutorial on how ANTLR works . Spark has written visitor functions to return the tree nodes while processing. Spark allows injection of a new parser instead of the default one. For supporting blob types, following things are done: Added ParserSessionExtension which is used to inject our parser Injected SparkSqlParserWithDataTypeExtension which replaces the default SparkSqlParser. This class replaces the default abstract syntax tree builder with AstBuilderWithDataTypeExtension AstBuilderWithDataTypeExtension overrides visitPrimitiveDataType and adds support for blob type Try it out $ bin/spark-shell --jars PATH /parser-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.parser.ParserSessionExtension scala spark.sql( create table tblWithBlob (mybytearray blob, id int) using parquet options (path '/tmp/p') ) scala spark.sql( Insert Into tblWithBlob values (array(1,2,3), 1) ) scala spark.sql( select * from tblwithblob ).schema res3: org.apache.spark.sql.types.StructType = StructType(StructField(mybytearray,ArrayType(ByteType,true),true), StructField(id,IntegerType,true)) scala spark.sql( select * from tblwithblob ).show +-----------+---+ |mybytearray| id| +-----------+---+ | [1, 2, 3]| 1| +-----------+---+ Github Here","title":"Parser Plugin"},{"location":"parser/#goal","text":"Support \"blob\" keyword in Apache Spark SQL. If someone specifies a \"blob\" keyword, it should be converted that to Array[Byte] internally.","title":"Goal"},{"location":"parser/#approach","text":"Apache Spark SQL parser is based on ANTLR. Here is a wonderful tutorial on how ANTLR works . Spark has written visitor functions to return the tree nodes while processing. Spark allows injection of a new parser instead of the default one. For supporting blob types, following things are done: Added ParserSessionExtension which is used to inject our parser Injected SparkSqlParserWithDataTypeExtension which replaces the default SparkSqlParser. This class replaces the default abstract syntax tree builder with AstBuilderWithDataTypeExtension AstBuilderWithDataTypeExtension overrides visitPrimitiveDataType and adds support for blob type","title":"Approach"},{"location":"parser/#try-it-out","text":"$ bin/spark-shell --jars PATH /parser-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.parser.ParserSessionExtension scala spark.sql( create table tblWithBlob (mybytearray blob, id int) using parquet options (path '/tmp/p') ) scala spark.sql( Insert Into tblWithBlob values (array(1,2,3), 1) ) scala spark.sql( select * from tblwithblob ).schema res3: org.apache.spark.sql.types.StructType = StructType(StructField(mybytearray,ArrayType(ByteType,true),true), StructField(id,IntegerType,true)) scala spark.sql( select * from tblwithblob ).show +-----------+---+ |mybytearray| id| +-----------+---+ | [1, 2, 3]| 1| +-----------+---+","title":"Try it out"},{"location":"parser/#github","text":"Here","title":"Github"},{"location":"planner/","text":"Goal Modify range operator so that it returns only the multiples of the step spark.range(start = 1, end = 10, step = 3, numPartitions = 2).show The actual range would return {1, 4, 7} The modified range should return {3, 6, 9} Approach Apache Spark's parser generates a abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. analysis, optimizer, planner and execution. In this exercise, I inject a planner rule that replaces a logical plan node with a physical plan node. There are multiple ways in which we change the Spark's range function but in this project I will inject a new Spark Plan for the range operator. Steps to achieve this: Add a PlannerExtension that injects our strategy Our strategy, ModifiedRangeExecStrategy, replaces the logical range operator with physical plan node ModifiedRangeExec. ModifiedRangeExec is a copy of RangeExec in spark except that I have changed the start number of the range. Since RangeExec is a physical plan node, it does the code generation. But let's keep that exercise for some other project. Try it out $ bin/spark-shell --jars PATH /planner-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.planner.PlannerExtension scala spark.range(start = 1, end = 10, step = 3, numPartitions = 2).show +---+ | id| +---+ | 3| | 6| | 9| +---+ scala spark.range(start = 2, end = 10, step = 3, numPartitions = 2).show +---+ | id| +---+ | 3| | 6| | 9| +---+ Github Here","title":"Planner Plugin"},{"location":"planner/#goal","text":"Modify range operator so that it returns only the multiples of the step spark.range(start = 1, end = 10, step = 3, numPartitions = 2).show The actual range would return {1, 4, 7} The modified range should return {3, 6, 9}","title":"Goal"},{"location":"planner/#approach","text":"Apache Spark's parser generates a abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. analysis, optimizer, planner and execution. In this exercise, I inject a planner rule that replaces a logical plan node with a physical plan node. There are multiple ways in which we change the Spark's range function but in this project I will inject a new Spark Plan for the range operator. Steps to achieve this: Add a PlannerExtension that injects our strategy Our strategy, ModifiedRangeExecStrategy, replaces the logical range operator with physical plan node ModifiedRangeExec. ModifiedRangeExec is a copy of RangeExec in spark except that I have changed the start number of the range. Since RangeExec is a physical plan node, it does the code generation. But let's keep that exercise for some other project.","title":"Approach"},{"location":"planner/#try-it-out","text":"$ bin/spark-shell --jars PATH /planner-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.planner.PlannerExtension scala spark.range(start = 1, end = 10, step = 3, numPartitions = 2).show +---+ | id| +---+ | 3| | 6| | 9| +---+ scala spark.range(start = 2, end = 10, step = 3, numPartitions = 2).show +---+ | id| +---+ | 3| | 6| | 9| +---+","title":"Try it out"},{"location":"planner/#github","text":"Here","title":"Github"},{"location":"resolver/","text":"Goal For a specific datasource, add an extra column that is derived from two other columns. Such a scenario can arise if you cannot change the datasource but you want to avoid manually adding that column every time. Approach Apache Spark's parser generates a abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. resolution, analysis, optimizer, planner and execution. In this exercise, we inject a resolver rule. This resolver rule adds a new column total salary based on the salary and bonus that are two other columns in the data source. ResolverExtension injects the rule AddMonthColumn The rule AddMonthColumn inserts a Project node in the existing plan tree with the new column. Try it out $ bin/spark-shell --jars PATH /resolver-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.resolver.ResolverExtension scala spark.sql( create table salary (id string, salary integer, bonus integer) using parquet options (path '/tmp/p') ) res0: org.apache.spark.sql.DataFrame = [] scala spark.sql( Insert Into salary values ('1', 100000, 10000) ) scala spark.sql( Insert Into salary values ('2', 200000, 15000) ) scala spark.sql( select * from salary ).show +---+------+-----+----------------------+ | id|salary|bonus|syscol_yearly_takeaway| +---+------+-----+----------------------+ | 1|100000|10000| 110000| | 2|200000|15000| 215000| +---+------+-----+----------------------+ Github Here","title":"Resolver Plugin"},{"location":"resolver/#goal","text":"For a specific datasource, add an extra column that is derived from two other columns. Such a scenario can arise if you cannot change the datasource but you want to avoid manually adding that column every time.","title":"Goal"},{"location":"resolver/#approach","text":"Apache Spark's parser generates a abstract syntax tree. The tree nodes are the operators of a query. The AST is passed through various phases of query compilation i.e. resolution, analysis, optimizer, planner and execution. In this exercise, we inject a resolver rule. This resolver rule adds a new column total salary based on the salary and bonus that are two other columns in the data source. ResolverExtension injects the rule AddMonthColumn The rule AddMonthColumn inserts a Project node in the existing plan tree with the new column.","title":"Approach"},{"location":"resolver/#try-it-out","text":"$ bin/spark-shell --jars PATH /resolver-0.1.0-SNAPSHOT.jar --conf spark.sql.extensions=dev.plugins.resolver.ResolverExtension scala spark.sql( create table salary (id string, salary integer, bonus integer) using parquet options (path '/tmp/p') ) res0: org.apache.spark.sql.DataFrame = [] scala spark.sql( Insert Into salary values ('1', 100000, 10000) ) scala spark.sql( Insert Into salary values ('2', 200000, 15000) ) scala spark.sql( select * from salary ).show +---+------+-----+----------------------+ | id|salary|bonus|syscol_yearly_takeaway| +---+------+-----+----------------------+ | 1|100000|10000| 110000| | 2|200000|15000| 215000| +---+------+-----+----------------------+","title":"Try it out"},{"location":"resolver/#github","text":"Here","title":"Github"},{"location":"smgr/","text":"Goal Plugin sort shuffle manager of your own with some extra logging. Approach Sort based shuffle as described in Spark's code comments : In sort-based shuffle, incoming records are sorted according to their target partition ids, then written to a single map output file. Reducers fetch contiguous regions of this file in order to read their portion of the map output. Shuffle extensively uses network and disk. There are storage specific ways in which one may want to optimize the network and disk i/o done by shuffle. For e.g. Apache Crail have modified Spark's shuffle to write intermediate data to their high performance storage. SortShuffleManager returns ShuffleWriter and ShuffleReader classes that can be implemented to introduce specific behavior. But, this is involved code. For the example here, I will just plugin a sort shuffle manager and a ShuffleWriter that does some extra logging while writing the map records. Following things were done in this project: Added a file SortShuffleManagerWithExtraLoggging. This returns our shuffle writer. Implemented ShuffleWriter that does some extra logging. Try it out Run the ShuffleManagerSpec. Github Here","title":"Shuffle Manager Plugin"},{"location":"smgr/#goal","text":"Plugin sort shuffle manager of your own with some extra logging.","title":"Goal"},{"location":"smgr/#approach","text":"Sort based shuffle as described in Spark's code comments : In sort-based shuffle, incoming records are sorted according to their target partition ids, then written to a single map output file. Reducers fetch contiguous regions of this file in order to read their portion of the map output. Shuffle extensively uses network and disk. There are storage specific ways in which one may want to optimize the network and disk i/o done by shuffle. For e.g. Apache Crail have modified Spark's shuffle to write intermediate data to their high performance storage. SortShuffleManager returns ShuffleWriter and ShuffleReader classes that can be implemented to introduce specific behavior. But, this is involved code. For the example here, I will just plugin a sort shuffle manager and a ShuffleWriter that does some extra logging while writing the map records. Following things were done in this project: Added a file SortShuffleManagerWithExtraLoggging. This returns our shuffle writer. Implemented ShuffleWriter that does some extra logging.","title":"Approach"},{"location":"smgr/#try-it-out","text":"Run the ShuffleManagerSpec.","title":"Try it out"},{"location":"smgr/#github","text":"Here","title":"Github"}]}